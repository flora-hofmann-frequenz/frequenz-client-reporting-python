{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Frequenz Reporting API Client","text":""},{"location":"#introduction","title":"Introduction","text":"<p>Reporting API client for Python</p> <p>TODO(cookiecutter): Improve the README file</p>"},{"location":"#supported-platforms","title":"Supported Platforms","text":"<p>The following platforms are officially supported (tested):</p> <ul> <li>Python: 3.11</li> <li>Operating System: Ubuntu Linux 20.04</li> <li>Architectures: amd64, arm64</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>If you want to know how to build this project and contribute to it, please check out the Contributing Guide.</p>"},{"location":"CONTRIBUTING/","title":"Contributing to Frequenz Reporting API Client","text":""},{"location":"CONTRIBUTING/#build","title":"Build","text":"<p>You can use <code>build</code> to simply build the source and binary distribution:</p> <pre><code>python -m pip install build\npython -m build\n</code></pre>"},{"location":"CONTRIBUTING/#local-development","title":"Local development","text":"<p>You can use editable installs to develop the project locally (it will install all the dependencies too):</p> <pre><code>python -m pip install -e .\n</code></pre> <p>Or you can install all development dependencies (<code>mypy</code>, <code>pylint</code>, <code>pytest</code>, etc.) in one go too: <pre><code>python -m pip install -e .[dev]\n</code></pre></p> <p>If you don't want to install all the dependencies, you can also use <code>nox</code> to run the tests and other checks creating its own virtual environments:</p> <pre><code>python -m pip install .[dev-noxfile]\nnox\n</code></pre> <p>You can also use <code>nox -R</code> to reuse the current testing environment to speed up test at the expense of a higher chance to end up with a dirty test environment.</p>"},{"location":"CONTRIBUTING/#running-tests-checks-individually","title":"Running tests / checks individually","text":"<p>For a better development test cycle you can install the runtime and test dependencies and run <code>pytest</code> manually.</p> <pre><code>python -m pip install .[dev-pytest]  # included in .[dev] too\n# And for example\npytest tests/test_*.py\n</code></pre> <p>Or you can use <code>nox</code>:</p> <pre><code>nox -R -s pytest -- test/test_*.py\n</code></pre> <p>The same appliest to <code>pylint</code> or <code>mypy</code> for example:</p> <pre><code>nox -R -s pylint -- test/test_*.py\nnox -R -s mypy -- test/test_*.py\n</code></pre>"},{"location":"CONTRIBUTING/#building-the-documentation","title":"Building the documentation","text":"<p>To build the documentation, first install the dependencies (if you didn't install all <code>dev</code> dependencies):</p> <pre><code>python -m pip install -e .[dev-mkdocs]\n</code></pre> <p>Then you can build the documentation (it will be written in the <code>site/</code> directory):</p> <pre><code>mkdocs build\n</code></pre> <p>Or you can just serve the documentation without building it using:</p> <pre><code>mkdocs serve\n</code></pre> <p>Your site will be updated live when you change your files (provided that you used <code>pip install -e .</code>, beware of a common pitfall of using <code>pip install</code> without <code>-e</code>, in that case the API reference won't change unless you do a new <code>pip install</code>).</p> <p>To build multi-version documentation, we use mike. If you want to see how the multi-version sites looks like locally, you can use:</p> <pre><code>mike deploy my-version\nmike set-default my-version\nmike serve\n</code></pre> <p><code>mike</code> works in mysterious ways. Some basic information:</p> <ul> <li><code>mike deploy</code> will do a <code>mike build</code> and write the results to your local <code>gh-pages</code> branch. <code>my-version</code> is an arbitrary name for the local version   you want to preview.</li> <li><code>mike set-default</code> is needed so when you serve the documentation, it goes to   your newly produced documentation by default.</li> <li><code>mike serve</code> will serve the contents of your local <code>gh-pages</code> branch. Be   aware that, unlike <code>mkdocs serve</code>, changes to the sources won't be shown   live, as the <code>mike deploy</code> step is needed to refresh them.</li> </ul> <p>Be careful not to use <code>--push</code> with <code>mike deploy</code>, otherwise it will push your local <code>gh-pages</code> branch to the <code>origin</code> remote.</p> <p>That said, if you want to test the actual website in your fork, you can always use <code>mike deploy --push --remote your-fork-remote</code>, and then access the GitHub pages produced for your fork.</p>"},{"location":"CONTRIBUTING/#releasing","title":"Releasing","text":"<p>These are the steps to create a new release:</p> <ol> <li> <p>Get the latest head you want to create a release from.</p> </li> <li> <p>Update the <code>RELEASE_NOTES.md</code> file if it is not complete, up to date, and    remove template comments (<code>&lt;!-- ... -&gt;</code>) and empty sections. Submit a pull    request if an update is needed, wait until it is merged, and update the    latest head you want to create a release from to get the new merged pull    request.</p> </li> <li> <p>Create a new signed tag using the release notes and    a semver compatible version number with a <code>v</code> prefix,    for example:</p> </li> </ol> <pre><code>git tag -s --cleanup=whitespace -F RELEASE_NOTES.md v0.0.1\n</code></pre> <ol> <li> <p>Push the new tag.</p> </li> <li> <p>A GitHub action will test the tag and if all goes well it will create    a GitHub    Release,    and upload a new package to    PyPI    automatically.</p> </li> <li> <p>Once this is done, reset the <code>RELEASE_NOTES.md</code> with the template:</p> </li> </ol> <pre><code>cp .github/RELEASE_NOTES.template.md RELEASE_NOTES.md\n</code></pre> <p>Commit the new release notes and create a PR (this step should be automated    eventually too).</p> <ol> <li>Celebrate!</li> </ol>"},{"location":"CONTRIBUTING/#cross-arch-testing","title":"Cross-Arch Testing","text":"<p>This project has built-in support for testing across multiple architectures. Currently, our CI conducts tests on <code>arm64</code> machines using QEMU emulation. We also have the flexibility to expand this support to include additional architectures in the future.</p> <p>This project contains Dockerfiles that can be used in the CI to test the python package in non-native machine architectures, e.g., <code>arm64</code>. The Dockerfiles exist in the directory <code>.github/containers/nox-cross-arch</code>, and follow a naming scheme so that they can be easily used in build matrices in the CI, in <code>nox-cross-arch</code> job. The naming scheme is:</p> <pre><code>&lt;arch&gt;-&lt;os&gt;-python-&lt;python-version&gt;.Dockerfile\n</code></pre> <p>E.g.,</p> <pre><code>arm64-ubuntu-20.04-python-3.11.Dockerfile\n</code></pre> <p>If a Dockerfile for your desired target architecture, OS, and python version does not exist here, please add one before proceeding to add your options to the test matrix.</p>"},{"location":"SUMMARY/","title":"SUMMARY","text":"<ul> <li>Home</li> <li>API Reference</li> <li>Contributing</li> </ul>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>frequenz<ul> <li>client<ul> <li>reporting</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/frequenz/client/reporting/","title":"reporting","text":""},{"location":"reference/frequenz/client/reporting/#frequenz.client.reporting","title":"frequenz.client.reporting","text":"<p>Client to connect to the Reporting API.</p> <p>This package provides a low-level interface for interacting with the reporting API.</p>"},{"location":"reference/frequenz/client/reporting/#frequenz.client.reporting-classes","title":"Classes","text":""},{"location":"reference/frequenz/client/reporting/#frequenz.client.reporting.ReportingApiClient","title":"frequenz.client.reporting.ReportingApiClient","text":"<p>A client for the Reporting service.</p> Source code in <code>frequenz/client/reporting/_client.py</code> <pre><code>class ReportingApiClient:\n\"\"\"A client for the Reporting service.\"\"\"\ndef __init__(self, service_address: str):\n\"\"\"Create a new Reporting client.\n        Args:\n            service_address: The address of the Reporting service.\n        \"\"\"\nself._grpc_channel = grpcaio.insecure_channel(service_address)\nself._stub = ReportingStub(self._grpc_channel)\n# pylint: disable=too-many-arguments\nasync def list_single_component_data(\nself,\n*,\nmicrogrid_id: int,\ncomponent_id: int,\nmetrics: Metric | list[Metric],\nstart_dt: datetime,\nend_dt: datetime,\npage_size: int = 1000,\n) -&gt; AsyncIterator[MetricSample]:\n\"\"\"Iterate over the data for a single metric.\n        Args:\n            microgrid_id: The microgrid ID.\n            component_id: The component ID.\n            metrics: The metric name or list of metric names.\n            start_dt: The start date and time.\n            end_dt: The end date and time.\n            page_size: The page size.\n        Yields:\n            A named tuple with the following fields:\n            * timestamp: The timestamp of the metric sample.\n            * value: The metric value.\n        \"\"\"\nasync for page in self._list_microgrid_components_data_pages(\nmicrogrid_components=[(microgrid_id, [component_id])],\nmetrics=[metrics] if isinstance(metrics, Metric) else metrics,\nstart_dt=start_dt,\nend_dt=end_dt,\npage_size=page_size,\n):\nfor entry in page:\nyield entry\n# pylint: disable=too-many-arguments\nasync def _list_microgrid_components_data_pages(\nself,\n*,\nmicrogrid_components: list[tuple[int, list[int]]],\nmetrics: list[Metric],\nstart_dt: datetime,\nend_dt: datetime,\npage_size: int = 1000,\n) -&gt; AsyncIterator[ComponentsDataPage]:\n\"\"\"Iterate over the pages of microgrid components data.\n        Note: This does not yet support resampling or aggregating the data. It\n        also does not yet support fetching bound and state data.\n        Args:\n            microgrid_components: A list of tuples of microgrid IDs and component IDs.\n            metrics: A list of metrics.\n            start_dt: The start date and time.\n            end_dt: The end date and time.\n            page_size: The page size.\n        Yields:\n            A ComponentsDataPage object of microgrid components data.\n        \"\"\"\nmicrogrid_components_pb = [\nPBMicrogridComponentIDs(microgrid_id=mid, component_ids=cids)\nfor mid, cids in microgrid_components\n]\ndef dt2ts(dt: datetime) -&gt; PBTimestamp:\nts = PBTimestamp()\nts.FromDatetime(dt)\nreturn ts\ntime_filter = PBTimeFilter(\nstart=dt2ts(start_dt),\nend=dt2ts(end_dt),\n)\nlist_filter = PBListMicrogridComponentsDataRequest.ListFilter(\ntime_filter=time_filter,\n)\nmetrics_pb = [metric.to_proto() for metric in metrics]\npage_token = None\nwhile True:\npagination_params = PBPaginationParams(\npage_size=page_size, page_token=page_token\n)\nresponse = await self._fetch_page(\nmicrogrid_components=microgrid_components_pb,\nmetrics=metrics_pb,\nlist_filter=list_filter,\npagination_params=pagination_params,\n)\nif not response or response.is_empty():\nbreak\nyield response\npage_token = response.next_page_token\nif not page_token:\nbreak\nasync def _fetch_page(\nself,\n*,\nmicrogrid_components: list[PBMicrogridComponentIDs],\nmetrics: list[PBMetric.ValueType],\nlist_filter: PBListMicrogridComponentsDataRequest.ListFilter,\npagination_params: PBPaginationParams,\n) -&gt; ComponentsDataPage | None:\n\"\"\"Fetch a single page of microgrid components data.\n        Args:\n            microgrid_components: A list of microgrid components.\n            metrics: A list of metrics.\n            list_filter: A list filter.\n            pagination_params: A pagination params.\n        Returns:\n            A ComponentsDataPage object of microgrid components data.\n        \"\"\"\ntry:\nrequest = PBListMicrogridComponentsDataRequest(\nmicrogrid_components=microgrid_components,\nmetrics=metrics,\nfilter=list_filter,\npagination_params=pagination_params,\n)\nresponse = await cast(\nAwaitable[PBListMicrogridComponentsDataResponse],\nself._stub.ListMicrogridComponentsData(request),\n)\nexcept grpcaio.AioRpcError as e:\nprint(f\"RPC failed: {e}\")\nreturn None\nreturn ComponentsDataPage(response)\nasync def close(self) -&gt; None:\n\"\"\"Close the client and cancel any pending requests immediately.\"\"\"\nawait self._grpc_channel.close(grace=None)\nasync def __aenter__(self) -&gt; \"ReportingApiClient\":\n\"\"\"Enter the async context.\"\"\"\nreturn self\nasync def __aexit__(\nself,\n_exc_type: Type[BaseException] | None,\n_exc_val: BaseException | None,\n_exc_tb: Any | None,\n) -&gt; bool | None:\n\"\"\"\n        Exit the asynchronous context manager.\n        Note that exceptions are not handled here, but are allowed to propagate.\n        Args:\n            _exc_type: Type of exception raised in the async context.\n            _exc_val: Exception instance raised.\n            _exc_tb: Traceback object at the point where the exception occurred.\n        Returns:\n            None, allowing any exceptions to propagate.\n        \"\"\"\nawait self.close()\nreturn None\n</code></pre>"},{"location":"reference/frequenz/client/reporting/#frequenz.client.reporting.ReportingApiClient-functions","title":"Functions","text":""},{"location":"reference/frequenz/client/reporting/#frequenz.client.reporting.ReportingApiClient.__aenter__","title":"__aenter__  <code>async</code>","text":"<pre><code>__aenter__() -&gt; ReportingApiClient\n</code></pre> <p>Enter the async context.</p> Source code in <code>frequenz/client/reporting/_client.py</code> <pre><code>async def __aenter__(self) -&gt; \"ReportingApiClient\":\n\"\"\"Enter the async context.\"\"\"\nreturn self\n</code></pre>"},{"location":"reference/frequenz/client/reporting/#frequenz.client.reporting.ReportingApiClient.__aexit__","title":"__aexit__  <code>async</code>","text":"<pre><code>__aexit__(\n_exc_type: Type[BaseException] | None,\n_exc_val: BaseException | None,\n_exc_tb: Any | None,\n) -&gt; bool | None\n</code></pre> <p>Exit the asynchronous context manager.</p> <p>Note that exceptions are not handled here, but are allowed to propagate.</p> PARAMETER  DESCRIPTION <code>_exc_type</code> <p>Type of exception raised in the async context.</p> <p> TYPE: <code>Type[BaseException] | None</code> </p> <code>_exc_val</code> <p>Exception instance raised.</p> <p> TYPE: <code>BaseException | None</code> </p> <code>_exc_tb</code> <p>Traceback object at the point where the exception occurred.</p> <p> TYPE: <code>Any | None</code> </p> RETURNS DESCRIPTION <code>bool | None</code> <p>None, allowing any exceptions to propagate.</p> Source code in <code>frequenz/client/reporting/_client.py</code> <pre><code>async def __aexit__(\nself,\n_exc_type: Type[BaseException] | None,\n_exc_val: BaseException | None,\n_exc_tb: Any | None,\n) -&gt; bool | None:\n\"\"\"\n    Exit the asynchronous context manager.\n    Note that exceptions are not handled here, but are allowed to propagate.\n    Args:\n        _exc_type: Type of exception raised in the async context.\n        _exc_val: Exception instance raised.\n        _exc_tb: Traceback object at the point where the exception occurred.\n    Returns:\n        None, allowing any exceptions to propagate.\n    \"\"\"\nawait self.close()\nreturn None\n</code></pre>"},{"location":"reference/frequenz/client/reporting/#frequenz.client.reporting.ReportingApiClient.__init__","title":"__init__","text":"<pre><code>__init__(service_address: str)\n</code></pre> <p>Create a new Reporting client.</p> PARAMETER  DESCRIPTION <code>service_address</code> <p>The address of the Reporting service.</p> <p> TYPE: <code>str</code> </p> Source code in <code>frequenz/client/reporting/_client.py</code> <pre><code>def __init__(self, service_address: str):\n\"\"\"Create a new Reporting client.\n    Args:\n        service_address: The address of the Reporting service.\n    \"\"\"\nself._grpc_channel = grpcaio.insecure_channel(service_address)\nself._stub = ReportingStub(self._grpc_channel)\n</code></pre>"},{"location":"reference/frequenz/client/reporting/#frequenz.client.reporting.ReportingApiClient.close","title":"close  <code>async</code>","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close the client and cancel any pending requests immediately.</p> Source code in <code>frequenz/client/reporting/_client.py</code> <pre><code>async def close(self) -&gt; None:\n\"\"\"Close the client and cancel any pending requests immediately.\"\"\"\nawait self._grpc_channel.close(grace=None)\n</code></pre>"},{"location":"reference/frequenz/client/reporting/#frequenz.client.reporting.ReportingApiClient.list_single_component_data","title":"list_single_component_data  <code>async</code>","text":"<pre><code>list_single_component_data(\n*,\nmicrogrid_id: int,\ncomponent_id: int,\nmetrics: Metric | list[Metric],\nstart_dt: datetime,\nend_dt: datetime,\npage_size: int = 1000\n) -&gt; AsyncIterator[MetricSample]\n</code></pre> <p>Iterate over the data for a single metric.</p> PARAMETER  DESCRIPTION <code>microgrid_id</code> <p>The microgrid ID.</p> <p> TYPE: <code>int</code> </p> <code>component_id</code> <p>The component ID.</p> <p> TYPE: <code>int</code> </p> <code>metrics</code> <p>The metric name or list of metric names.</p> <p> TYPE: <code>Metric | list[Metric]</code> </p> <code>start_dt</code> <p>The start date and time.</p> <p> TYPE: <code>datetime</code> </p> <code>end_dt</code> <p>The end date and time.</p> <p> TYPE: <code>datetime</code> </p> <code>page_size</code> <p>The page size.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> YIELDS DESCRIPTION <code>AsyncIterator[MetricSample]</code> <p>A named tuple with the following fields:</p> <code>AsyncIterator[MetricSample]</code> <ul> <li>timestamp: The timestamp of the metric sample.</li> </ul> <code>AsyncIterator[MetricSample]</code> <ul> <li>value: The metric value.</li> </ul> Source code in <code>frequenz/client/reporting/_client.py</code> <pre><code>async def list_single_component_data(\nself,\n*,\nmicrogrid_id: int,\ncomponent_id: int,\nmetrics: Metric | list[Metric],\nstart_dt: datetime,\nend_dt: datetime,\npage_size: int = 1000,\n) -&gt; AsyncIterator[MetricSample]:\n\"\"\"Iterate over the data for a single metric.\n    Args:\n        microgrid_id: The microgrid ID.\n        component_id: The component ID.\n        metrics: The metric name or list of metric names.\n        start_dt: The start date and time.\n        end_dt: The end date and time.\n        page_size: The page size.\n    Yields:\n        A named tuple with the following fields:\n        * timestamp: The timestamp of the metric sample.\n        * value: The metric value.\n    \"\"\"\nasync for page in self._list_microgrid_components_data_pages(\nmicrogrid_components=[(microgrid_id, [component_id])],\nmetrics=[metrics] if isinstance(metrics, Metric) else metrics,\nstart_dt=start_dt,\nend_dt=end_dt,\npage_size=page_size,\n):\nfor entry in page:\nyield entry\n</code></pre>"}]}